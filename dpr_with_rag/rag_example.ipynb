{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, RagRetriever, RagModel\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "# retriever = RagRetriever.from_pretrained(\n",
    "#     \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n",
    "# )\n",
    "# # initialize with RagRetriever to do everything in one forward call\n",
    "# model = RagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n",
    "\n",
    "# inputs = tokenizer(\"what is capital city of French?\", return_tensors=\"pt\")\n",
    "# outputs = model(input_ids=inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18 + 4 + 13 + 3 + 12 + 14 + 13 + 4 + 24 + 19 + 14 + 0 + 11 + 8 + 2 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "# retriever = RagRetriever.from_pretrained(\n",
    "#     \"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True\n",
    "# )\n",
    "# # initialize with RagRetriever to do everything in one forward call\n",
    "# model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n",
    "# targets = tokenizer(text_target=\"In Paris, there are 10 million people.\", return_tensors=\"pt\")\n",
    "# input_ids = inputs[\"input_ids\"]\n",
    "# labels = targets[\"input_ids\"]\n",
    "# outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "# # or use retriever separately\n",
    "# model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", use_dummy_dataset=True)\n",
    "# # 1. Encode\n",
    "# question_hidden_states = model.question_encoder(input_ids)[0]\n",
    "# # 2. Retrieve\n",
    "# docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=\"pt\")\n",
    "# doc_scores = torch.bmm(\n",
    "#     question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)\n",
    "# ).squeeze(1)\n",
    "# # 3. Forward to generator\n",
    "\n",
    "\n",
    "# generated = model.generate(\n",
    "#     context_input_ids=docs_dict[\"context_input_ids\"],\n",
    "#     context_attention_mask=docs_dict[\"context_attention_mask\"],\n",
    "#     doc_scores=doc_scores,\n",
    "# )\n",
    "# generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, RagRetriever, RagTokenForGeneration\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "# retriever = RagRetriever.from_pretrained(\n",
    "#     \"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True\n",
    "# )\n",
    "# # initialize with RagRetriever to do everything in one forward call\n",
    "# model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n",
    "# targets = tokenizer(text_target=\"In Paris, there are 10 million people.\", return_tensors=\"pt\")\n",
    "# input_ids = inputs[\"input_ids\"]\n",
    "# labels = targets[\"input_ids\"]\n",
    "# outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "# # or use retriever separately\n",
    "# model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", use_dummy_dataset=True)\n",
    "# # 1. Encode\n",
    "# question_hidden_states = model.question_encoder(input_ids)[0]\n",
    "# # 2. Retrieve\n",
    "# docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=\"pt\")\n",
    "# doc_scores = torch.bmm(\n",
    "#     question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)\n",
    "# ).squeeze(1)\n",
    "# # 3. Forward to generator\n",
    "# outputs = model(\n",
    "#     context_input_ids=docs_dict[\"context_input_ids\"],\n",
    "#     context_attention_mask=docs_dict[\"context_attention_mask\"],\n",
    "#     doc_scores=doc_scores,\n",
    "#     decoder_input_ids=labels,\n",
    "# )\n",
    "\n",
    "# # or directly generate\n",
    "# generated = model.generate(\n",
    "#     context_input_ids=docs_dict[\"context_input_ids\"],\n",
    "#     context_attention_mask=docs_dict[\"context_attention_mask\"],\n",
    "#     doc_scores=doc_scores,\n",
    "# )\n",
    "# generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# print(generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, RagRetriever, RagTokenForGeneration\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "# retriever = RagRetriever.from_pretrained(\n",
    "#     \"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True\n",
    "# )\n",
    "# # initialize with RagRetriever to do everything in one forward call\n",
    "# model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# inputs = tokenizer(\"Who is Trump?\", return_tensors=\"pt\")\n",
    "# targets = tokenizer(text_target=\"In China, there are 10 million people.\", return_tensors=\"pt\")\n",
    "# input_ids = inputs[\"input_ids\"]\n",
    "# labels = targets[\"input_ids\"]\n",
    "# outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "# # or use retriever separately\n",
    "# model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", use_dummy_dataset=True)\n",
    "# # 1. Encode\n",
    "# question_hidden_states = model.question_encoder(input_ids)[0]\n",
    "# # 2. Retrieve\n",
    "# docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=\"pt\")\n",
    "# doc_scores = torch.bmm(\n",
    "#     question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)\n",
    "# ).squeeze(1)\n",
    "# # 3. Forward to generator\n",
    "# outputs = model(\n",
    "#     context_input_ids=docs_dict[\"context_input_ids\"],\n",
    "#     context_attention_mask=docs_dict[\"context_attention_mask\"],\n",
    "#     doc_scores=doc_scores,\n",
    "#     decoder_input_ids=labels,\n",
    "# )\n",
    "\n",
    "# # or directly generate\n",
    "# generated = model.generate(\n",
    "#     context_input_ids=docs_dict[\"context_input_ids\"],\n",
    "#     context_attention_mask=docs_dict[\"context_attention_mask\"],\n",
    "#     doc_scores=doc_scores,\n",
    "# )\n",
    "# generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# print(generated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 위 코드 좀 이상함....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "\n",
    "# tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "# retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "# model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# input_dict = tokenizer.prepare_seq2seq_batch(\"who is the president of USA?\", return_tensors=\"pt\") \n",
    "\n",
    "# generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "# print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration \n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "#     tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\") \n",
    "#     retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True) \n",
    "#     model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever) \n",
    "\n",
    "#     input_dict = tokenizer.prepare_seq2seq_batch(\"how many countries are in europe?\", return_tensors=\"pt\") \n",
    "\n",
    "#     generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "#     print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pip install faiss-cpu --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "\n",
    "# tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "# retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "# model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# input_dict = tokenizer.prepare_seq2seq_batch(\"where is capital city of France?\", return_tensors=\"pt\") \n",
    "\n",
    "# generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "# print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for research block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leadawon5/decs_jupyter_lab/venvs/bartvenv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Found cached dataset wiki_dpr (/root/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.no_index-dummy=True,with_index=False/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n",
      "Found cached dataset wiki_dpr (/root/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.exact-df1b7a7f4307b5db/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n",
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.weight', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/leadawon5/decs_jupyter_lab/venvs/bartvenv/lib/python3.7/site-packages/transformers/models/rag/tokenization_rag.py:92: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  FutureWarning,\n",
      "/home/leadawon5/decs_jupyter_lab/venvs/bartvenv/lib/python3.7/site-packages/transformers/generation/utils.py:1358: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Antarctic Treaty System / The Antarctic Treaty System's yearly \"Antarctic Treaty Consultative Meetings (ATCM)\" are the international forum for the administration and management of the region. Only 29 of the 53 parties to the agreements have the right to participate in decision-making at these meetings, though the other 24 are still allowed to attend. The decision-making participants are the \"Consultative Parties\" and, in addition to the 12 original signatories, include 17 countries that have demonstrated their interest in Antarctica by carrying out substantial scientific activity there. As of 2015, there are 53 states party to the treaty, 29 of which, including all 12 original // how many countries are in europe?\n",
      " Asia / Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Asia is subdivided into 48 countries, three of them (Russia, Kazakhstan and Turkey) having part of their land in Europe. Asia has extremely diverse climates and geographic features. Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia. It is moist across southeast sections, and dry across much of the interior. Some of the largest daily temperature // how many countries are in europe?\n",
      " Antarctic Treaty System / Antarctic Treaty System The Antarctic Treaty and related agreements, collectively known as the Antarctic Treaty System (ATS), regulate international relations with respect to Antarctica, Earth's only continent without a native human population. For the purposes of the treaty system, Antarctica is defined as all of the land and ice shelves south of 60°S latitude. The treaty entered into force in 1961 and currently has 53 parties. The treaty sets aside Antarctica as a scientific preserve, establishes freedom of scientific investigation, and bans military activity on the continent. The treaty was the first arms control agreement established during the Cold War. // how many countries are in europe?\n",
      " Antarctic Treaty System / South African residents and members of expeditions organised in South Africa. Antarctic Treaty System The Antarctic Treaty and related agreements, collectively known as the Antarctic Treaty System (ATS), regulate international relations with respect to Antarctica, Earth's only continent without a native human population. For the purposes of the treaty system, Antarctica is defined as all of the land and ice shelves south of 60°S latitude. The treaty entered into force in 1961 and currently has 53 parties. The treaty sets aside Antarctica as a scientific preserve, establishes freedom of scientific investigation, and bans military activity on the continent. The treaty // how many countries are in europe?\n",
      " Anatolia / Anatolia Anatolia (from Greek '; \"east\" or \"[sun]rise\"), also known as Asia Minor (Medieval and Modern Greek: ', \"small Asia\"; ), Asian Turkey, the Anatolian peninsula, or the Anatolian plateau, is the westernmost protrusion of Asia, which makes up the majority of modern-day Turkey. The region is bounded by the Black Sea to the north, the Mediterranean Sea to the south, the Armenian Highlands to the east, and the Aegean Sea to the west. The Sea of Marmara forms a connection between the Black and Aegean Seas through the Bosphorus and Dardanelles straits and separates Anatolia from Thrace on the // how many countries are in europe?\n",
      " 54\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2979836/815854861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# index error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/leadawon5/decs_jupyter_lab/venvs/bartvenv/lib/python3.7/site-packages/transformers/models/rag/tokenization_rag.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/leadawon5/decs_jupyter_lab/venvs/bartvenv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3511\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         )\n\u001b[1;32m   3515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/leadawon5/decs_jupyter_lab/venvs/bartvenv/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         clean_up_tokenization_spaces = (\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration \n",
    "# import inspect\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\") \n",
    "# retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True) \n",
    "# model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever) \n",
    "\n",
    "# input_dict = tokenizer.prepare_seq2seq_batch(\"how many countries are in europe?\", return_tensors=\"pt\") \n",
    "\n",
    "# generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "# print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) \n",
    "# print(tokenizer.batch_decode(generated, skip_special_tokens=True)[1])  # index error\n",
    "# print(tokenizer.batch_decode(generated, skip_special_tokens=True)[2]) \n",
    "# print(tokenizer.batch_decode(generated, skip_special_tokens=True)[3]) \n",
    "# print(tokenizer.batch_decode(generated, skip_special_tokens=True)[4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))  # index error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Found cached dataset wiki_dpr (/root/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.no_index-dummy=True,with_index=False/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n",
      "Found cached dataset wiki_dpr (/root/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.exact-df1b7a7f4307b5db/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n",
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.weight', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Asia / Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Asia is subdivided into 48 countries, three of them (Russia, Kazakhstan and Turkey) having part of their land in Europe. Asia has extremely diverse climates and geographic features. Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia. It is moist across southeast sections, and dry across much of the interior. Some of the largest daily temperature // how many countries are in asia?\n",
      " Asia / religions including Christianity, Islam, Judaism, Hinduism, Buddhism, Confucianism, Taoism, Jainism, Sikhism, Zoroastrianism, as well as many other religions. Given its size and diversity, the concept of Asia—a name dating back to classical antiquity—may actually have more to do with human geography than physical geography. Asia varies greatly across and within its regions with regard to ethnic groups, cultures, environments, economics, historical ties and government systems. It also has a mix of many different climates ranging from the equatorial south via the hot desert in the Middle East, temperate areas in the east and the continental centre to vast subarctic and // how many countries are in asia?\n",
      " Asia / across much of the Middle East. The Yangtze River in China is the longest river in the continent. The Himalayas between Nepal and China is the tallest mountain range in the world. Tropical rainforests stretch across much of southern Asia and coniferous and deciduous forests lie farther north. A survey carried out in 2010 by global risk analysis farm Maplecroft identified 16 countries that are extremely vulnerable to climate change. Each nation's vulnerability was calculated using 42 socio, economic and environmental indicators, which identified the likely climate change impacts during the next 30 years. The Asian countries of Bangladesh, India, // how many countries are in asia?\n",
      " Asia / and the Balkans from the mid 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing dynasty. The Islamic Mughal Empire and the Hindu Maratha Empire controlled much of India in the 16th and 18th centuries respectively. Asia is the largest continent on Earth. It covers 9% of the Earth's total surface area (or 30% of its land area), and has the largest coastline, at. Asia is generally defined as comprising the eastern four-fifths of Eurasia. It is located to the east of the Suez Canal and the Ural Mountains, and south of the // how many countries are in asia?\n",
      " Asia / Christian sects mainly adhered to Assyrian people or Syriac Christians. Saint Thomas Christians in India trace their origins to the evangelistic activity of Thomas the Apostle in the 1st century. Islam, which originated in the Hejaz located in modern-day Saudi Arabia, is the second largest and most widely-spread religion in Asia with at least 1 billion Muslims constituting around 23.8% of the total population of Asia. With 12.7% of the world Muslim population, the country currently with the largest Muslim population in the world is Indonesia, followed by Pakistan (11.5%), India (10%), Bangladesh, Iran and Turkey. Mecca, Medina and Jerusalem // how many countries are in asia?\n",
      " 48\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration \n",
    "import inspect\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\") \n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True) \n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever) \n",
    "\n",
    "input_dict = tokenizer.prepare_seq2seq_batch(\"how many countries are in asia?\", return_tensors=\"pt\") \n",
    "\n",
    "generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 2929,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]])\n",
      " 48\n"
     ]
    }
   ],
   "source": [
    "print(generated)\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))  # index error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Found cached dataset wiki_dpr (/root/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.no_index-dummy=True,with_index=False/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n",
      "Found cached dataset wiki_dpr (/root/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.exact-df1b7a7f4307b5db/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n",
      "Some weights of the model checkpoint at facebook/rag-sequence-nq were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.weight', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Apple Inc. / A mid-October 2013 announcement revealed that Burberry executive Angela Ahrendts will commence as a senior vice president at Apple in mid-2014. Ahrendts oversaw Burberry's digital strategy for almost eight years and, during her tenure, sales increased to about US$3.2 billion and shares gained more than threefold. Alongside Google vice-president Vint Cerf and AT&T CEO Randall Stephenson, Cook attended a closed-door summit held by President Obama on August 8, 2013, in regard to government surveillance and the Internet in the wake of the Edward Snowden NSA incident. On February 4, 2014, Cook met with Abdullah Gül, the President of Turkey, in // who is apple ceo?\n",
      " Apple Inc. / not have a chairman and instead had two co-lead directors, Andrea Jung and Arthur D. Levinson, who continued with those titles until Levinson became chairman of the board in November. On October 5, 2011, Steve Jobs died, marking the end of an era for Apple. The first major product announcement by Apple following Jobs's passing occurred on January 19, 2012, when Apple's Phil Schiller introduced iBooks Textbooks for iOS and iBook Author for Mac OS X in New York City. Jobs had stated in his biography that he wanted to reinvent the textbook industry and education. From 2011 to 2012, // who is apple ceo?\n",
      " Apple Inc. / campus on October 15, 2013, after a 2011 presentation by Jobs detailing the architectural design of the new building and its environs. The new campus is planned to house up to 13,000 employees in one central, four-storied, circular building surrounded by extensive landscape. It will feature a café with room for 3,000 sitting people and parking underground as well as in a parking structure. The 2.8million square foot facility will also include Jobs's original designs for a fitness center and a corporate auditorium. Apple has expanded its campuses in Austin, Texas concurrently with building Apple Park in Cupertino. The expansion // who is apple ceo?\n",
      " Apple Inc. / consists of two locations, with one having 1.1million square feet of workspace, and the other 216,000 square feet. Apple will invest $1 billion to build the North Austin campus. At the biggest location, 6,000 employees work on technical support, manage Apple's network of suppliers to fulfill product shipments, aid in maintaining iTunes Store and App Store, handle economy, and continuously update Apple Maps with new data. At its smaller campus, 500 engineers work on next-generation processor chips to run in future Apple products. Apple's headquarters for Europe, the Middle East and Africa (EMEA) are located in Cork in the south // who is apple ceo?\n",
      " Apple Inc. / to bring Jobs back. Jobs regained leadership within the company and became the new CEO shortly after. He began to rebuild Apple's status, opening Apple's own retail stores in 2001, acquiring numerous companies to create a portfolio of software titles, and changing some of the hardware used in its computers. The company returned to profitability. In January 2007, Jobs renamed the company Apple Inc., reflecting its shifted focus toward consumer electronics, and announced the iPhone, which saw critical acclaim and significant financial success. In August 2011, Jobs resigned as CEO due to health complications, and Tim Cook became the new // who is apple ceo?\n",
      " tim cook\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration \n",
    "import inspect\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\") \n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True) \n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever) \n",
    "\n",
    "input_dict = tokenizer.prepare_seq2seq_batch(\"who is apple CEO?\", return_tensors=\"pt\") \n",
    "\n",
    "generated = model.generate(input_ids=input_dict[\"input_ids\"]) \n",
    "print(tokenizer.batch_decode(generated, skip_special_tokens=True)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 15679,  7142,     2,     1,     1,     1,     1]])\n",
      " tim cook\n"
     ]
    }
   ],
   "source": [
    "print(generated)\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))  # index error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bartvenv",
   "language": "python",
   "name": "bartvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
