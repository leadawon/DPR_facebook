{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leadawon5/decs_jupyter_lab/venvs/dprvenv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizerFast, AlbertModel\n",
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1, model2):\n",
    "  for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train-v2.0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1243900/2517782116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_squad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-v2.0.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mval_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_squad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev-v2.0.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1243900/2517782116.py\u001b[0m in \u001b[0;36mread_squad\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_squad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0msquad_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train-v2.0.json'"
     ]
    }
   ],
   "source": [
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two â€“ fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model_name = \"facebook/dpr-question_encoder-single-nq-base\"\n",
    "passage_model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    "\n",
    "query_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(query_model_name)\n",
    "passage_tokenizer = DPRContextEncoderTokenizer.from_pretrained(passage_model_name)\n",
    "query_model = DPRQuestionEncoder.from_pretrained(query_model_name)\n",
    "passage_model = DPRContextEncoder.from_pretrained(passage_model_name)\n",
    "\n",
    "query_model.train()\n",
    "passage_model.train()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_encodings = query_tokenizer(train_questions, truncation=True, padding=True, return_tensors = 'pt')\n",
    "train_context_encodings = passage_tokenizer(train_contexts, truncation=True, padding=True, return_tensors = 'pt')\n",
    "\n",
    "val_query_encodings = query_tokenizer(val_questions, truncation=True, padding=True, return_tensors = 'pt')\n",
    "val_context_encodings = passage_tokenizer(val_contexts, truncation=True, padding=True, return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPR(nn.Module):\n",
    "\n",
    "\n",
    "  def __init__(self, query_model, passage_model, query_tokenizer, passage_tokenizer, \n",
    "              dense_size, freeze_params = 0.0, batch_size = 2, sample_size = 4):\n",
    "    \n",
    "    '''\n",
    "    :query_model : The model that encodes queries to dense representation\n",
    "    :passage_model : The model that encodes passages to dense representation\n",
    "    :query_tokenizer : tokenizer for queries\n",
    "    :passage_tokenizer : tokenizer for passages\n",
    "    :passage_dict : dictionary of passages with their unique id\n",
    "    :questions : A list of tuples with question and their correct passage id\n",
    "    :dense_size : the dimension to which the DPR has to encode\n",
    "    :freeze_params : the percentage of the parameters to be frozen\n",
    "    :batch_size : the batch size for training\n",
    "    :sample_size: the sample size for negative sampling\n",
    "    '''\n",
    "    super(DPR, self).__init__()\n",
    "    self.query_model = query_model\n",
    "    self.query_tokenizer = query_tokenizer\n",
    "    self.passage_model = passage_model\n",
    "    self.passage_tokenizer = passage_tokenizer\n",
    "    self.freeze_params = freeze_params\n",
    "    self.sample_size = sample_size\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    self.passage_to_dense = nn.Sequential(nn.Linear(768, dense_size * 2),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(dense_size * 2, dense_size),\n",
    "                                          nn.GELU())\n",
    "    \n",
    "    self.query_to_dense = nn.Sequential(nn.Linear(768, dense_size * 2),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(dense_size * 2, dense_size),\n",
    "                                          nn.GELU())\n",
    "    self.log_softmax = nn.LogSoftmax(dim = 1)\n",
    "    self.freeze_layers()\n",
    "\n",
    "\n",
    "  # Freeze the first self.freeze_params % layers\n",
    "  def freeze_layers(self):\n",
    "    num_query_layers = sum(1 for _ in self.query_model.parameters())\n",
    "    num_passage_layers = sum(1 for _ in self.passage_model.parameters())\n",
    "\n",
    "    for parameters in list(self.query_model.parameters())[:int(self.freeze_params * num_query_layers)]:\n",
    "      parameters.requires_grad = False\n",
    "\n",
    "    for parameters in list(self.query_model.parameters())[int(self.freeze_params * num_query_layers):]:\n",
    "      parameters.requires_grad = True\n",
    "\n",
    "    for parameters in list(self.passage_model.parameters())[:int(self.freeze_params * num_passage_layers)]:\n",
    "      parameters.requires_grad = False\n",
    "\n",
    "    for parameters in list(self.passage_model.parameters())[int(self.freeze_params * num_passage_layers):]:\n",
    "      parameters.requires_grad = True\n",
    "\n",
    "  def get_passage_vectors(self, passage):\n",
    "    p_vector = self.passage_model(input_ids = passage.input_ids, \n",
    "                                  attention_mask = passage.attention_mask)\n",
    "    p_vector = self.query_to_dense(p_vector.pooler_output)\n",
    "    return p_vector\n",
    "\n",
    "  def get_query_vector(self, query):\n",
    "    q_vector = self.query_model(input_ids = query.input_ids, \n",
    "                                attention_mask = query.attention_mask)\n",
    "    q_vector = self.query_to_dense(q_vector.pooler_output)\n",
    "    return q_vector\n",
    "\n",
    "  def dot_product(self, q_vector, p_vector):\n",
    "    q_vector = q_vector.unsqueeze(1)\n",
    "    sim = torch.matmul(q_vector, torch.transpose(p_vector, -2, -1))\n",
    "    return sim\n",
    "\n",
    "  def forward(self, context_input_ids, context_attention_mask, query_input_ids, query_attention_mask):\n",
    "    dense_passage = self.passage_model(input_ids = context_input_ids, attention_mask = context_attention_mask)\n",
    "    dense_query = self.query_model(input_ids = query_input_ids, attention_mask = query_attention_mask)\n",
    "    dense_passage = dense_passage['pooler_output']\n",
    "    dense_query = dense_query['pooler_output']\n",
    "    dense_passage = self.passage_to_dense(dense_passage)\n",
    "    dense_query = self.query_to_dense(dense_query)\n",
    "    similarity_score = self.dot_product(dense_query, dense_passage)\n",
    "    similarity_score = similarity_score.squeeze(1)\n",
    "    logits = self.log_softmax(similarity_score)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpr_model = DPR(query_model = query_model, \n",
    "                passage_model = passage_model, \n",
    "                query_tokenizer = query_tokenizer, \n",
    "                passage_tokenizer = passage_tokenizer, \n",
    "                dense_size = 64,\n",
    "                freeze_params = 0.3,\n",
    "                batch_size = 2,\n",
    "                sample_size = 8)\n",
    "\n",
    "dpr_model.train()\n",
    "\n",
    "\n",
    "sum(p.numel() for p in dpr_model.parameters()), sum(p.numel() for p in dpr_model.parameters() if p.requires_grad == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.AdamW(dpr_model.parameters(), lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "neg_samples = 2\n",
    "num_questions = len(train_context_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "  true = []\n",
    "  context_input_ids_tensor = []\n",
    "  context_attention_mask_tensor = []\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  true.append(1)\n",
    "\n",
    "  ## Selecting the query and its positive context\n",
    "  idx = random.randint(0, num_questions)\n",
    "  context_input_ids = train_context_encodings.input_ids[idx]\n",
    "  context_attention_mask = train_context_encodings.attention_mask[idx]\n",
    "  query_input_ids = train_query_encodings.input_ids[idx]\n",
    "  query_attention_mask = train_query_encodings.attention_mask[idx]\n",
    "  context_input_ids_tensor.append(context_input_ids)\n",
    "  context_attention_mask_tensor.append(context_attention_mask)\n",
    "\n",
    "  ## Selecting the negative contexts which is hardcoded as 100 index from the selected positive context\n",
    "\n",
    "  for j in range(neg_samples):\n",
    "    true.append(0)\n",
    "    neg_idx_1 = idx + 100\n",
    "    neg_idx_2 = idx - 100\n",
    "    if neg_idx_1 < num_questions:\n",
    "      context_input_ids = train_context_encodings.input_ids[neg_idx_1]\n",
    "      context_attention_mask = train_context_encodings.attention_mask[neg_idx_1]\n",
    "      context_input_ids_tensor.append(context_input_ids)\n",
    "      context_attention_mask_tensor.append(context_attention_mask)\n",
    "\n",
    "    elif neg_idx_2 > 0:\n",
    "      context_input_ids = train_context_encodings.input_ids[neg_idx_2]\n",
    "      context_attention_mask = train_context_encodings.attention_mask[neg_idx_2]\n",
    "      context_input_ids_tensor.append(context_input_ids)\n",
    "      context_attention_mask_tensor.append(context_attention_mask)\n",
    "\n",
    "  \n",
    "  context_input_ids_tensor = torch.stack(context_input_ids_tensor)\n",
    "  context_attention_mask_tensor = torch.stack(context_attention_mask_tensor)\n",
    "  query_input_ids = query_input_ids.unsqueeze(0)\n",
    "  query_attention_mask = query_attention_mask.unsqueeze(0)\n",
    "  return context_input_ids_tensor, context_attention_mask_tensor, query_input_ids, query_attention_mask, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = 0\n",
    "for i in range(1, 500):\n",
    "  context_input_ids_tensor, context_attention_mask_tensor,query_input_ids,query_attention_mask, true = get_batch()\n",
    "  pred = dpr_model(context_input_ids_tensor, context_attention_mask_tensor,query_input_ids,query_attention_mask)\n",
    "  \n",
    "  true = torch.tensor([0])\n",
    "  #print(pred.size(), true.size())\n",
    "  loss = criterion(pred, true)\n",
    "  loss.backward()\n",
    "  batch_loss += loss.item()\n",
    "  optimizer.step()\n",
    "  if i%20 == 0:\n",
    "    print(f\"Batch : {int(i/20)}  Loss : {batch_loss/20}\")\n",
    "    batch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./content/drive/MyDrive/Haystack/dpr_3.pt\"\n",
    "torch.save(dpr_model, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dprvenv",
   "language": "python",
   "name": "dprvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
